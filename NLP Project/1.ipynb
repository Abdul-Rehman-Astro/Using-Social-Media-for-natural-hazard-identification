{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# use spacy as object of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "easiest\n",
      "to\n",
      "install\n",
      "Git\n",
      "on\n",
      "Linux\n",
      "using\n",
      "the\n",
      "preferred\n",
      "package\n",
      "manager\n",
      "of\n",
      "your\n",
      "Linux\n",
      "distribution\n",
      ".\n",
      "It is easiest to install Git on Linux using the preferred package manager of your Linux distribution.\n",
      "If\n",
      "you\n",
      "prefer\n",
      "to\n",
      "build\n",
      "from\n",
      "source\n",
      ".\n",
      "If you prefer to build from source.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It is easiest to install Git on Linux using the preferred package manager of your Linux distribution. If you prefer to build from source.\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is easiest to install Git on Linux using the preferred package manager of your Linux distribution.',\n",
       " 'If you prefer to build from source, you can find tarballs on kernel.org.',\n",
       " 'The latest version is 2.41.0.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"It is easiest to install Git on Linux using the preferred package manager of your Linux distribution. If you prefer to build from source, you can find tarballs on kernel.org. The latest version is 2.41.0. \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Exercise\n",
    "\n",
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
    "\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[http://www.data.gov/,\n",
       " http://www.science,\n",
       " http://data.gov.uk/.,\n",
       " http://www3.norc.org/gss+website/,\n",
       " http://www.europeansocialsurvey.org/.]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: token has an attribute that can be used to detect a url\n",
    "\n",
    "\n",
    "doc= nlp(text)\n",
    "\n",
    "urls = []\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        urls.append(token)\n",
    "\n",
    "urls\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "(2) Extract all money transaction from below sentence along with currency. Output should be,\n",
    "\n",
    "two $\n",
    "\n",
    "500 €\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "transactions = '''Tony gave two $ to Peter, Bruce gave 500 € to Steve'''\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: Use token.i for the index of a token and token.is_currency for currency symbol detection\n",
    "\n",
    "money =[]\n",
    "\n",
    "doc = nlp(transactions)\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i +1].is_currency:\n",
    "        print(token.text, doc[token.i +1].text)\n",
    "\n",
    "# money\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3412536205.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_80896/3412536205.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    token.i???\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "token.i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7fcb51de3fa0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7fcb51de3e20>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7fcb52053a50>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7fcb51d55680>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7fcb51ddb6c0>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7fcb520539e0>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
